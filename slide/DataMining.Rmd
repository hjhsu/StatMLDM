---
title       : Data Mining
author      : Wush Wu
framework   : io2012-wush
highlighter : highlight.js
hitheme     : zenburn
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
mode        : selfcontained # {standalone, draft}
knit        : slidify::knit2slides
geometry: margin=0in
--- .largecontent &vcenter

```{r setup, include=FALSE, cache=FALSE}
library(knitr)
library(magrittr)
library(data.table)
library(dplyr)
library(ggplot2)
library(quantmod)
library(jsonlite)
library(arules)
library(animation)
library(FNN)
library(magrittr)
library(plotrix)
library(fpc)

opts_chunk$set(echo = FALSE, cache=TRUE, comment="",
               cache.path = "cache-DataMining/",
               dev.args=list(bg="transparent"),
               fig.path = "./assets/fig/rdata-mining-",
               fig.width = 10, fig.height = 6)
bg <- function(path) sprintf("bg:url(assets/img/%s)", path)
fig <- function(path, size = 100) {
  sprintf("<img src='assets/img/%s' style='max-width: %d%%;max-height: %d%%'></img>",
          path, size, size)
}
fig2 <- function(path, size = 100) {
  sprintf("<img src='assets/img/%s' style='width: %d%%'></img>",
          path, size)
}
sys_name <- Sys.info()["sysname"] %>% tolower
sys_encode <- c("utf8", "utf8", "big5")[pmatch(sys_name, c("linux", "darwin", "windows"))]
```

## 課程大綱

- 什麼是Data Mining
- Frequency Pattern Mining
- Similarity and Distance
- Clustering
- Classification
- Text Mining

--- .dark .segue

## 什麼是Data Mining

--- &fullimg `r bg("myn8a.jpg")`

Source: <http://gold.sarwatch.org/reports/gold-mining-orientale-province>

--- &fullimg `r bg("dm.gif")`

Source: <http://www.laits.utexas.edu/~anorman/BUS.FOR/course.mat/Alex>

--- &vcenter .largecontent

## Data Mining v.s. Machine Learning v.s. Statistics

- Data Mining: Find the value from the data 
- Machine Learning: Learn the pattern from the data
- Statistics: Making inference based on assumptions

--- .dark .segue

## Frequency Pattern Mining

--- &vcenter .largecontent

## 歸納

- 交易紀錄
    - \{milk, bread\}
    - \{butter\}
    - \{beer, diapers\}
    - \{milk, bread, butter\}
    - \{bread\}
- 規則
    - milk ==> bread

--- &vcenter .largecontent

## Frequency Pattern Mining

- Transaction(交易紀錄)
    - 一名顧客所購買的物品清單：\{milk, bread\}
- Item
    - 可購買的物品：milk、break、beer...
- Itemset
    - 可購買的物品集合，例如：\{milk, break\}

--- &vcenter .largecontent

## Frequency Pattern Mining

- Rule
    - 物品之間的關聯：$X$ ==> $Y$（$X$, $Y$又被稱為itemset）
    - $X$(LHS itemset) 和 $Y$(RHS itemset) 不能包含相同的物品
    - $X$ => $Y$ 代表當 $X$ 出現在一個transaction時，$Y$也會出現
    - 例： milk ==> bread

--- &fullimg `r bg("what-is-big-data-19-638.jpg")`

<http://cloud-network-master.blogspot.tw/2013/07/bigdata.html>

--- &fullimg `r bg("Visitor-Profile.png")`

<http://piwik.org/docs/real-time/>

--- &fullimg `r bg("MONK.png")`

<http://www.gosugamers.net/hearthstone/news/27121-all-decklists-from-seatstory-cup>

--- &vcenter .largecontent

## Mining by Computer

- 規則成立的門檻
    - 交易紀錄
        - \{milk, bread\}
        - \{butter\}
        - \{beer, diapers\}
        - \{milk, bread, butter\}
        - \{bread\}
    - 規則
        - milk ==> bread (100%, 2/5)
        - beer ==> diapers (100%, 1/5)
        - milk, bread ==> butter (50%, 2/5)

--- &vcenter .largecontent

## Support

- 給定 itemset $X$, 有多少個比率的Transaction包含 $X$ 稱為 $Supp(X)$
- 交易紀錄
    - \{milk, bread\}
    - \{beer, diapers\}
    - \{milk, bread, butter\}
    - \{bread\}
- $Supp(\{\text{milk}\}) = \frac{2}{4}$ 
- $Supp(\{\text{milk, bread}\}) = \frac{2}{4}$

--- &vcenter .largecontent

## Confidence

- 給定 rule $X \Rightarrow Y$, $Conf(X \Rightarrow Y) = \frac{Supp(X \cup Y)}{Supp(X)}$
- 交易紀錄
    - \{milk, bread\}
    - \{milk\}
    - \{milk, bread, butter\}
    - \{butter\}
- $Conf(\{\text{bread => milk}\}) = \frac{2}{2}$ 
- $Conf(\{\text{milk => bread}\}) = \frac{2}{3}$

--- &vcenter .largecontent

## Frequency Pattern Mining

- 規則的Confidence要明顯 => 準
- 規則的Support要高 => 有影響

--- &vcenter .largecontent

## $Supp$ 和 $Conf$ 的不足

- 交易紀錄
    - \{milk, bread\}
    - \{milk\}
    - \{milk, bread, butter\}
    - \{bread\}
- milk => bread: 
    - $Supp(milk) = 75\%$, $Conf(milk \Rightarrow bread) = 67\%$
- $Supp(bread) = 75\%$: 所以要促銷麵包的時候，需要推廣牛奶嗎？

--- &vcenter .largecontent

## Lift

- $Lift(X \Rightarrow Y) = \frac{Supp(X \cup Y)}{Supp(X) \times Supp(Y)} = \frac{Conf(X \Rightarrow Y)}{Supp(Y)}$
- 交易紀錄
    - \{milk, bread\}
    - \{milk\}
    - \{milk, bread, butter\}
    - \{bread\}
- $Lift(milk \Rightarrow bread) = \frac{\frac{2}{4}}{\frac{3}{4} \times \frac{3}{4}} = \frac{8}{9}$

--- &vcenter .largecontent

## Frequency Pattern Mining

- 規則的Confidence要明顯 => 準
- 規則的Support要高 => 有影響
- 規則的Lift要高 => 與背景比較後提升的程度

--- &vcenter .largecontent

## 練習

- 請完成RDataMining-01-Association-Rule

--- .dark .segue

## Clustering and Similarity

--- &fullimg `r bg("main-qimg-d9f961a492f073fbfed7202ef5badf17.gif")`

<http://qr.ae/RU5Xx6>

--- &vcenter .largecontent

## 叢集分析

- 物以類聚
- 近朱者赤、近墨者黑
- 叢集分析的基礎：資料之間的距離、資料之間的相似度

--- &fullimg `r bg("company-cluster.png")`

--- &fullimg `r bg("evolutionary_tree.jpg")`

<http://www.bio.miami.edu/dana/160/160S13_5.html>

--- .dark .segue

## 「相似度」與「相異度」

--- &vcenter .largecontent

## 相似度(Similarity)與相異度(Dissimilarity)

- 相似度：越大越像
- 相異度：越小越像
    - 距離
- 兩者可以用數學運算做變換
    - 常見的相似度常常介於0與1之間
    - 定義相異度： 1 - 相似度

--- &vcenter .largecontent

## 二元類別型變數

- $p = (1, 0, 0, 0, 0)$, q = $(1, 1, 0, 1, 0)$
    - $M_{11} = 1, M_{00} = 2$
    - $M_{10} = 0, M_{01} = 2$
- Simple Matching Coefficient(SMC): $\frac{M_{11} + M_{00}}{M_{01} + M_{10} + M_{11} + M_{00}}$
- Jaccard Index: $\frac{M_{11}}{M_{01} + M_{10} + M_{11}}$

--- &vcenter .largecontent

## 標籤 ==> Jaccard Index

- 使用者A : \{男性、單身、電玩、上班族\}
- 使用者B : \{男性、旅遊、電玩、學生\}
- Jaccard Index: $\frac{\left| A \cap B \right|}{\left| A \cup B \right|}$
    - J(使用者A, 使用者B) ： $\frac{2}{6}$
    - $v := \{i_{男性}, i_{單身}, i_{電玩}, i_{上班族}, i_{旅遊}, i_{學生}, i_{女性}, ...\}$
    - $v_A = \{1, 1, 1, 1, 0, 0, 0, ...\}$
    - $v_B = \{1, 0, 1, 0, 1, 1, 0, ...\}$

--- &fullimg `r bg("709544_0.jpg")`

<http://www.twwiki.com/wiki/%E6%8B%9B%E6%A8%99>

--- &vcenter .largecontent

## 利用爬蟲技術收集決標資料

- <http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmAwardAction.do?newEdit=false&searchMode=common&method=inquiryForPublic&pkAtmMain=1126170&tenderCaseNo=931116-2>
- <http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmAwardAction.do?newEdit=false&searchMode=common&method=inquiryForPublic&pkAtmMain=1495141&tenderCaseNo=CG590C>
- <http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmAwardAction.do?newEdit=false&searchMode=common&method=inquiryForPublic&pkAtmMain=1501992&tenderCaseNo=EA95029M008>
- <http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmAwardAction.do?newEdit=false&searchMode=common&method=inquiryForPublic&pkAtmMain=1518749&tenderCaseNo=KHC1521-9515>
- <http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmAwardAction.do?newEdit=false&searchMode=common&method=inquiryForPublic&pkAtmMain=1799822&tenderCaseNo=TPC-MS-FAB-S2>
- <http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmAwardAction.do?newEdit=false&searchMode=common&method=inquiryForPublic&pkAtmMain=1854786&tenderCaseNo=960625>

--- &vcenter .largecontent

## 利用爬蟲技術取得公司董監事名單

|id       |name                             |parent |birthday   |magnate                       |
|:--------|:--------------------------------|:------|:----------|:-----------------------------|
|00000000 |復華廣告有限公司                 |NA     |1976-05-24 |                              |
|00000016 |富台機械開發建設有限公司         |NA     |1979-04-30 |王振林                        |
|00000022 |泰煜建材股份有限公司             |NA     |NA         |                              |
|00000037 |茂盛工程有限公司（同名）         |NA     |1978-07-08 |                              |
|00000043 |啟猛股份有限公司（無統編）       |NA     |1984-05-22 |鄭添發                        |
|00000058 |詠詳鐵工廠股份有限公司（無統蝙） |NA     |1984-03-07 |吳秋進,吳戴麗珍,謝素梅,吳秋龍 |

--- &vcenter .largecontent

## 利用Jaccard Index 計算公司董監事相似度

- <http://web.pcc.gov.tw/tps/main/pms/tps/atm/atmAwardAction.do?newEdit=false&searchMode=common&method=inquiryForPublic&pkAtmMain=51493408&tenderCaseNo=GF4-103122>

|        |id       |name                     |parent |birthday   |magnate                     |
|:-------|:--------|:------------------------|:------|:----------|:---------------------------|
|555426  |27229231 |尚達塩業股份有限公司     |NA     |2005-05-30 |吳秀里,周永紹,周博元,周碩良 |
|1067348 |70794974 |上達糧業國際股份有限公司 |NA     |2002-01-08 |吳秀里,周永紹,周博元,周碩良 |

- Jaccard Index 為 1

--- &vcenter .largecontent

## 數值型變數：Cosine Similarity

- $X_1, X_2 \in \mathbb{R}^d$
- Cosine Similarity： $\frac {X_1 \cdot X_2}{\left\lVert X_1 \right\rVert \left\lVert X_2 \right\rVert}$
- `r fig("unit_circle.gif")`
    - <small style="font-size: 50%"><https://www.math.hmc.edu/calculus/tutorials/reviewtriglogexp/></small>

--- &vcenter .largecontent

## 數值型變數：Correlation

- $X_1, X_2 \in \mathbb{R}^d$
- Let $X_1' = \frac{X_1 - mean(X_1)}{sd(X_1)}$, $X_2' = \frac{X_2 - mean(X_2)}{sd(X_2)}$, Correlation: $X_1' \cdot X_2'$
- `r fig("correlation.jpeg")`
    - <small style="font-size: 50%"><https://www.biomedware.com/files/documentation/spacestat/interface/Views/Correlation_Coefficients.htm></small>

--- &vcenter .largecontent

## 距離（相異度）

- $d(x, y)$
    - $d(x, y) \geq 0$：距離一定是正的
    - $d(x, y) = 0 \Leftrightarrow x = y$：相同的物品 $\Leftrightarrow$距離為0
    - $d(x, y) = d(y, x)$：距離是對稱的
    - $d(x, z) \leq d(x, y) + d(y, z)$：三角不等式

--- &vcenter .largecontent

## 數值型變數： $L_p$距離

- $X_1 = (x_{1,1}, x_{1, 2}, ..., x_{1, d})  \in \mathbb{R}^d$
- $X_2 = (x_{2, 1}, x_{2, 2}, ..., x_{2, d}) \in \mathbb{R}^d$
- $L_p(X_1, X_2) = \left( \sum_{k = 1}^d {\left| x_{1, k} - x_{2, k} \right|^p} \right)^{1/p}$

--- .largecontent

## 數值型變數： Manhattan Distance, $L_1$

$$d(i,j) = 
  \left\lVert x_{i,1} - x_{j,1} \right\rVert +
  \left\lVert x_{i,2} - x_{j,2} \right\rVert +
  \dots +
  \left\lVert x_{i,p} - x_{j,p} \right\rVert$$

<center>
```{r L1, fig.width = 7, fig.height = 7}
xlim <- ylim <- c(-1.2, 1.2)
plot(0, 0, type = "n", xlim = xlim, ylim = ylim,
     xlab = "", ylab = "", xaxt = "n", yaxt = "n", 
     bty = "n")
f <- function(x) 1 - abs(x)
curve(f, -1, 1, add = TRUE, lwd = 2)
f <- function(x) - (1 - abs(x))
curve(f, -1, 1, add = TRUE, lwd = 2)
arrows(
  c(0,xlim[1]),
  c(ylim[1],0),
  c(0,xlim[2]),
  c(ylim[2],0),
  0.10)
```
</center>

--- .largecontent

## 數值型變數： Euclidean Distance, $L_2$

$$d(i,j) = \sqrt{
  \left( x_{i,1} - x_{j,1} \right)^2 +
  \left( x_{i,2} - x_{j,2} \right)^2 +
  \dots +
  \left( x_{i,p} - x_{j,p} \right)^2
}$$

<center>
```{r L2, fig.width = 7, fig.height = 7}
xlim <- ylim <- c(-1.2, 1.2)
plot(0, 0, type = "n", xlim = xlim, ylim = ylim,
     xlab = "", ylab = "", xaxt = "n", yaxt = "n", 
     bty = "n")
f <- function(x) sqrt(1 - x^2)
curve(f, -1, 1, add = TRUE, lwd = 2)
f <- function(x) - sqrt(1 - x^2)
curve(f, -1, 1, add = TRUE, lwd = 2)
arrows(
  c(0,xlim[1]),
  c(ylim[1],0),
  c(0,xlim[2]),
  c(ylim[2],0),
  0.10)
```
</center>

--- .largecontent

## 數值型變數： Maximum Distance, $L_{\infty}$

$$d(i, j) = max_{k=1}^p {
  \left\lVert x_{i,k} - x_{j,k} \right\rVert
}$$

<center>
```{r Linfty, fig.width = 7, fig.height = 7}
xlim <- ylim <- c(-1.2, 1.2)
plot(0, 0, type = "n", xlim = xlim, ylim = ylim,
     xlab = "", ylab = "", xaxt = "n", yaxt = "n", 
     bty = "n")
lines(c(-1, 1), c(1, 1), lwd = 2)
lines(c(-1, 1), c(-1, -1), lwd = 2)
lines(c(1, 1), c(-1, 1), lwd = 2)
lines(c(-1, -1), c(-1, 1), lwd = 2)
arrows(
  c(0,xlim[1]),
  c(ylim[1],0),
  c(0,xlim[2]),
  c(ylim[2],0),
  0.10)
```
</center>

--- &vcenter .largecontent

## 數值型變數 Minkowski Distance

$$d(i,j) =
\left(
  \left\lVert x_{i,1} - x_{j,1} \right\rVert^q +
  \left\lVert x_{i,2} - x_{j,2} \right\rVert^q +
  \dots +
  \left\lVert x_{i,p} - x_{j,p} \right\rVert^q
\right)$$

<center>
```{r Minkowski, fig.width = 7, fig.height = 7, fig.show = "hide", results = "hide"}
xlim <- ylim <- c(-1.2, 1.2)
q.list <- c(seq(0.2, 2, by = 0.2),
            seq(2.4, 4, by = 0.4),
            8, 16, 32)
out <- "minkowski.gif"
suppressMessages(saveGIF({
  for(i in seq_along(q.list)) {
    q <- q.list[i]
    dev.hold()
    plot(0, 0, type = "n", xlim = xlim, ylim = ylim,
         xlab = "", ylab = "", xaxt = "n", yaxt = "n", 
         bty = "n", main = sprintf("q = %0.1f", q),
         cex.main = 4)
    arrows(c(0,xlim[1]), c(ylim[1],0),
           c(0,xlim[2]), c(ylim[2],0),
           0.10)
    f1 <- function(x) (1 - abs(x)^q)^(1/q)
    curve(f1, -1, 1, add = TRUE, lwd = 2, n = 1001)
    f2 <- function(x) -f1(x)
    curve(f2, -1, 1, add = TRUE, lwd = 2, n = 1001)
    ani.pause()
  }
  plot(0, 0, type = "n", xlim = xlim, ylim = ylim,
       xlab = "", ylab = "", xaxt = "n", yaxt = "n", 
       bty = "n", main = sprintf("q = Inf"),
       cex.main = 4)
  arrows(c(0,xlim[1]), c(ylim[1],0),
         c(0,xlim[2]), c(ylim[2],0),
         0.10)
  lines(c(-1, 1), c(1, 1), lwd = 2)
  lines(c(-1, 1), -c(1, 1), lwd = 2)
  lines(c(1, 1), c(-1, 1), lwd = 2)
  lines(-c(1, 1), c(-1, 1), lwd = 2)
  ani.pause()
}, out))
```
<img src="minkowski.gif"/>
</center>

--- &vcenter .largecontent

## Gower's Dissimilarity coefficient

- $x_i = (x_{i,1}, x_{i,2}, x_{i,3}, ..., x_{i,p})$
- $x_j = (x_{j,1}, x_{j,2}, x_{j,3}, ..., x_{j,p})$
- 如果第一個變數是順序尺度或數值變數
    - $s_{i,j,1} = \frac{\left\lVert x_{i,1} - x_{j,1} \right\rVert}{r_1}$
        - $r_1$代表第一個變數的range
    - $0 \leq s_{i,j,1} \leq 1$

--- &vcenter .largecontent

## Gower's Dissimilarity Coefficient

- $x_i = (x_{i,1}, x_{i,2}, x_{i,3}, ..., x_{i,p})$
- $x_j = (x_{j,1}, x_{j,2}, x_{j,3}, ..., x_{j,p})$
- 如果第二個變數是類別變數
    - $s_{i,j,2}$ 則是 $1 - $Jaccard Index
    - $0 \leq s_{i,j,2} \leq 1$

--- &vcenter .largecontent

## Gower's Dissimilarity Coefficient

- $s_{i,j} = \frac{\sum_{k=1}^p {w_k s_{i,j,p}}}{\sum_{k=1}^p {w_k}}$
    - Gower's Similarity coefficient 就是這些similarity指標的weighted sum

--- &vcenter .largecontent

## Gower's Dissimilarity Coefficient

```{r gower, echo = TRUE, eval = FALSE}
library(cluster)
d <- daisy(iris, metric = "gower")
```

--- &vcenter .largecontent

## 如何挑選Similarity

- 視目標與應用而定
    - 利用1NN 分類的結果來評估Similarity的品質(後述)
- 數值變數的baseline：
    - 先標準化
    - $L_2$距離

--- .dark .segue

## Hierarchical Clustering

--- &vcenter .largecontent

## Dendrogram

- 決定資料點之間的距離
    - 將相鄰的資料點合併成一個Cluster
- 決定資料點與Cluster之間的距離
- 決定Cluster與Cluster之間的距離
- 由近到遠依序合併資料點與Clusters...

```{r hclust-figs, fig.show = "hide", warning = FALSE, cache = TRUE}
m <- cl$merge
d.m <- as.matrix(dist(iris2))
row_group <- list()
draw_group <- function(node, col, df = iris2) {
  node.m <- apply(df[node,], 2, mean)
  points(Sepal.Width ~ Sepal.Length, df[node,], col = col)
  points(node.m[1], node.m[2], col = col, pch = 16)
}
draw_line <- function(node, col, df = iris2) {
  lines(df[node,1], df[node,2], lty = 2, col = col, lwd = 3)
  line.dist <- sqrt(diff(df[node,1])^2 + diff(df[node,2])^2)
  line.center <- c(mean(df[node,1]), mean(df[node,2]))
  text.location <- line.center
  text.location[2] <- text.location[2] + diff(par()$usr[3:4]) / 25
  title(sub = sprintf("Distance: %0.3f", line.dist))
}
col.set <- rainbow(11)
plot(Sepal.Width ~ Sepal.Length, iris2, pch = 16)
y.label.offset <- rep(-diff(par()$usr[3:4]) / 25, nrow(iris2))
y.label.offset[8] <- -y.label.offset[8]
text(
  iris2$Sepal.Length, 
  iris2$Sepal.Width + y.label.offset,
  rownames(iris2))

node.closest <- local({
  diag(d.m) <- Inf
  which(d.m == min(d.m), arr.ind = TRUE)[1,]
}) %>%
  draw_line(col = col.set[2])
for(i in 1:11) {
  lhs <- m[i,1]
  rhs <- m[i,2]
  lnode <- if (lhs < 0) -lhs else {
    tmp <- row_group[[lhs]]
    row_group[[lhs]] <- integer(0)
    tmp
  }
  rnode <- if (rhs < 0) -rhs else {
    tmp <- row_group[[rhs]]
    row_group[[rhs]] <- integer(0)
    tmp
  }
  node <- c(lnode, rnode)
  row_group[[i]] <- node
  bnode <- setdiff(1:12, Reduce(union, row_group[1:i]))
  plot(Sepal.Width ~ Sepal.Length, iris2, type = "n")
  points(Sepal.Width ~ Sepal.Length, iris2[bnode,], pch = 16)
  text(
    iris2$Sepal.Length, 
    iris2$Sepal.Width + y.label.offset,
    rownames(iris2))
  lapply(seq_len(i-1), function(j) {
    if (length(j) > 0) draw_group(row_group[[j]], col.set[j])
  }) %>% 
    invisible()
  draw_group(node, col = col.set[i])
  if (i < 11) {
    plot(Sepal.Width ~ Sepal.Length, iris2, type = "n")
    points(Sepal.Width ~ Sepal.Length, iris2[bnode,], pch = 16)
  text(
    iris2$Sepal.Length, 
    iris2$Sepal.Width + y.label.offset,
    rownames(iris2))
    lapply(seq_len(i), function(j) {
      if (length(j) > 0) draw_group(row_group[[j]], col.set[j])
    }) %>% 
      invisible()
    local({
      d.m <- 
        (df <- rbind(
          iris2[bnode,],
          Filter(function(x) length(row_group[[x]]) > 0, seq_len(i)) %>%
            (function(df, i) df[i])(df = row_group) %>%
          lapply(function(j) {
            iris2[j,] %>% apply(2, mean)
          }) %>% do.call(what = rbind))) %>%
        dist() %>%
        as.matrix()
      diag(d.m) <- Inf
      which(d.m == min(d.m), arr.ind = TRUE)[1,] %>%
        draw_line(col = col.set[i], df = df)
    }) 
  }
}
```

```{r hclust-figs-display, dependson="hclust-figs", results = "asis"}
fig.list <- dir("assets/fig", "rdata-mining-hclust-figs-.*.png$")
fig.id <- 
  regmatches(fig.list, 
           regexec("^rdata-mining-hclust-figs-(\\d+).png$",
                   fig.list)) %>%
  sapply("[", 2) %>%
  as.integer()
fig.order <- order(fig.id)
lapply(
  fig.list[fig.order], 
  function(fig.path) {
    fig.path <- file.path("..", "fig", fig.path)
    sprintf("\n--- &fullimg %s\n", bg(fig.path)) %>% cat()
  }
) %>% invisible()
```

--- &vcenter .largecontent

## Dendrogram

```{r dendrogram}
iris2 <- iris[c(1:4, 51:54, 101:104),1:2]
d <- dist(iris2)
cl <- hclust(d)
plot(cl, lwd = 2, xlab = "", ylab = "", sub = "", cex.main = 2, cex = 1.5, cex.axis = 2)
```

--- &vcenter .largecontent

## Hierarchical Clustering

- 給定Dendrogram, 如果要找出k個Cluster，就使用當全部資料被分成k個Cluster的瞬間當成結果

```{r dendrogram2}
iris2 <- iris[c(1:4, 51:54, 101:104),1:2]
d <- dist(iris2)
cl <- hclust(d)
plot(cl, lwd = 2, xlab = "", ylab = "", sub = "", cex.main = 2, cex = 1.5, cex.axis = 2)
abline(h = tail(cl$height, 3)[1] + 0.05, lty = 2)
text(5, tail(cl$height, 3)[1] + 0.15, "k = 3", cex = 1.5)
```

--- &vcenter .largecontent

## 如何評斷Clustering結果的好壞？

- Cluster之內的距離要短
- Cluster之間的距離要長

--- &vcenter .largecontent

## 如何挑選Cluster的個數?

- 挑選Clustering結果好的Cluster個數
- 透過dendrogram的高度差距來比較

--- .dark .segue

## Center-based Clustering

```{r kmeans-figs, fig.show = "hide", warning = FALSE, cache = TRUE}
x <- model.matrix(Species ~ Sepal.Length + Sepal.Width - 1, iris)
centers <- 3
d <- dist(x)
gr.center <- sample(seq_len(nrow(x)), centers, FALSE)
gr <- sample(seq_len(centers), nrow(x), TRUE)
x.center <- x[gr.center,]
dst <- matrix(nrow = nrow(x), ncol = centers)
j <- 1
pch <- as.integer(iris$Species)
col <- 1:3
is.continue <- TRUE
while(is.continue) {
  plot(x, pch = pch, col = col[gr], main = "k-means")
  points(x.center, pch = 16, col = 1:3,
         cex = 3, lwd = 2)
  knn <- get.knnx(x.center, x, k = 1)
  gr.last <- gr
  gr <- as.vector(knn$nn.index)
  if (isTRUE(all.equal(gr, gr.last))) is.continue <- FALSE
  x.center <- split(seq_len(nrow(x)), gr) %>%
    lapply(function(k) {
      apply(x[k,], 2, mean)
    }) %>%
    do.call(what = rbind)
}
```

```{r kmeans-figs-display, dependson="kmeans-figs", results="asis"}
fig.list <- dir("assets/fig", "rdata-mining-kmeans-figs-.*.png$")
fig.id <- 
  regmatches(fig.list, 
           regexec("^rdata-mining-kmeans-figs-(\\d+).png$",
                   fig.list)) %>%
  sapply("[", 2) %>%
  as.integer()
fig.order <- order(fig.id)
lapply(
  fig.list[fig.order], 
  function(fig.path) {
    fig.path <- file.path("..", "fig", fig.path)
    sprintf("\n--- &fullimg %s\n", bg(fig.path)) %>% cat()
  }
) %>% invisible()
```

--- &vcenter .largecontent

## K-Means Clustering

- 資料間的距離
- 中心點的個數(與起始值)

--- &vcenter .largecontent

## 如何評估k-means的分群結果好壞？

```{r withinss}
d <- dist(iris[,-5])
result <- 
  lapply(2:10, function(i) {
    .x <- sapply(1:20, function(j) {
      cl <- kmeans(d, i)
      sum(cl$withinss)
    })
    data.frame(withinss = mean(.x), k = i)
  }) %>%
  do.call(what = rbind)
plot(withinss ~ k, result, cex.lab = 1.8)
lines(withinss ~ k, result)
```

--- &vcenter .largecontent

## Within-Cluster Sum of Squares

```{r withinss-display}
iris2 <- iris[c(1,2,51,52,101,102),1:2]
cl <- kmeans(iris2, centers = 2)
plot(iris2, main = "Within-Cluster Sum of Squares", col = cl$cluster, pch = cl$cluster)
points(cl$centers, col = 1:2, pch = 16:17)
for(i in seq_len(nrow(iris2))) {
  start <- iris2[i,]
  end <- cl$centers[cl$cluster[i],]
  tmp <- rbind(start, end)
  lines(tmp[,1], tmp[,2], col = cl$cluster[i])
}
```

--- &vcenter .largecontent

## Gap Statistic for Estimating the Number of Clusters

```{r gap-cluster}
library(cluster)
gap <- clusGap(iris[,-5], kmeans, 10, B = 100, verbose = interactive())
plot(gap, cex.lab = 1.5)
```

--- .dark .segue

## Density-based Clustering

```{r fpc-figs, results="hide", warning=FALSE, cache = TRUE, fig.show="hide"}
x <- model.matrix(Species ~ Sepal.Length + Sepal.Width - 1, iris)
d <- as.matrix(dist(x))
g <- dbscan(x, 0.2)
cl <- integer(nrow(iris))
i.all <- seq_len(nrow(iris))
i.next <- c()
i.done <- c()
get_next_i <- function(i.next, i.done) {
  cat(sprintf("i.next: %s i.done: %s \n", paste(i.next, collapse = ","), paste(i.done, collapse = ",")))
  if (length(i.next) == 0) setdiff(i.all, i.done) %>% head(1) else i.next[1]
}
i <- get_next_i(i.next, i.done)
while(length(i) == 1) {
  if (length(targets <- which(d[i,] < g$eps)) >= g$MinPts) {
    if (cl[i] != 0 & all(cl[targets] == cl[i])) {
    } else {
      dev.hold()
      plot(x[,1], x[,2], pch = pch, col = cl + 1, main = "dbscan", type = "p")
      ani.pause()
      dev.hold()
      plot(x[,1], x[,2], pch = pch, col = cl + 1, main = "dbscan", type = "p")
      draw.circle(x[i,1], x[i,2], radius = g$eps, col = rgb(0.5, 0.5, 0.5, 0.5), lty = 2)
      ani.pause()
    }
    if (cl[i] == 0) cl[i] <- max(cl) + 1
    if (cl[i] == 4) stop("")
    cl[targets] <- cl[i]
    i.done <- append(i.done, i)
    i.next <- union(i.next, targets) %>% setdiff(i.done)
    i <- get_next_i(i.next, i.done)
    i.next <- setdiff(i.next, i)
  } else {
    i.done <- append(i.done, i)
    i <- get_next_i(i.next, i.done)
    i.next <- setdiff(i.next, i)
  }
}
```

```{r fpc-figs-display, dependson="fpc-figs", results="asis"}
fig.list <- dir("assets/fig", "rdata-mining-fpc-figs-.*.png$")
fig.id <- 
  regmatches(fig.list, 
           regexec("^rdata-mining-fpc-figs-(\\d+).png$",
                   fig.list)) %>%
  sapply("[", 2) %>%
  as.integer()
fig.order <- order(fig.id)
lapply(
  fig.list[fig.order], 
  function(fig.path) {
    fig.path <- file.path("..", "fig", fig.path)
    sprintf("\n--- &fullimg %s\n", bg(fig.path)) %>% cat()
  }
) %>% invisible()
```

--- &vcenter .largecontent

## DBSCAN

- 資料間的距離
- Reachability distance（判斷有沒有連接）
- Reachability minimum number of points （判斷是不是雜訊）

--- &vcenter .largecontent

## 分群總結

- 分群就是:
    - 定義資料間的距離（Similarity）
    - 套用分群演算法

--- &vcenter .largecontent

## 分群比較

```{r compare-clustering, fig.width = 14, fig.height = 7, cache = TRUE}
op <- par(mar = rep(0, 4))
layout(matrix(1:9, ncol = 3))
plot2 <- function(x, col) {
  plot(x, col = col, type = "p", xaxt = "n", yaxt = "n")
}
plot3 <- function(text) {
  plot(1, 1, type = "n", xaxt = "n", yaxt = "n")
  text(1, 1, text, cex = 2.5)
}
x1 <- local({
  n <- 500
  retval1 <- 
    runif(n, 0, 2*pi) %>%
    Map(f = function(x) c(sin(x), cos(x))) %>%
    do.call(what = rbind) 
  retval1 <- retval1 + rnorm(2 * n, 0, 0.1)
  retval2 <- 
    runif(n, 0, 2*pi) %>%
    Map(f = function(x) 2 * c(sin(x), cos(x))) %>%
    do.call(what = rbind) 
  retval2 <- retval2 + rnorm(2 * n, 0, 0.1)
  rbind(retval1, retval2)
})
# x1.col <- rep(1:2, each = nrow(x1) / 2)
# plot2(x1, col = x1.col)
x2 <- local({
  rbind(
    matrix(rnorm(200, 0, 0.1), ncol = 2),
    matrix(rnorm(200, 1, 0.1), ncol = 2),
    matrix(rnorm(200, 2, 0.1), ncol = 2)
  )
})
# x2.col <- rep(1:3, each = 100)
# plot2(x2, col = x2.col)

plot3("Hierarchical Clustering")
cl.h1 <- hclust(dist(x1))
plot2(x1, col = cutree(cl.h1, 2))
cl.h2 <- hclust(dist(x2))
plot2(x2, col = cutree(cl.h2, 3))

plot3("Center-based Clustering")
cl.k1 <- kmeans(x1, centers = 2)
plot2(x1, col = cl.k1$cluster)
points(cl.k1$centers, pch = 17, col = 1:2, cex = 4)
cl.k2 <- kmeans(x2, centers = 3)
plot2(x2, col = cl.k2$cluster)
points(cl.k2$centers, pch = 17, col = 1:3, cex = 4)

plot3("Density-based Clustering")
cl.d1 <- dbscan(x1, 0.3, MinPts = 3)
plot2(x1, col = cl.d1$cluster)
cl.d2 <- dbscan(x2, 0.3)
plot2(x2, col = cl.d2$cluster)
invisible(dev.off())
par(op)
```

--- &vcenter .largecontent

## 練習

- 請完成RDataMining-02-Clustering

--- .dark .segue

## Similarity and Classification

--- &vcenter .largecontent

## Classification

- Training Datset:
    - 每個資料點均有： 屬性 $X$, 標籤（類別型變數） $Y$
- Testing Dataset: 
    - 在僅觀察到 $X$ 的狀態下去預測 $Y$
- Logistic Regression / SVM / Decision Tree / Gradient Boosted Decision Tree...

--- &vcenter .largecontent

## Nearest-Neighborhood

- 給一筆Testing data，找與它最近的Training data（鄰居）
- 用鄰居的類別猜測Testing data的類別

```{r knn}
set.seed(1)
.i <- sample(seq_len(150), 100) %>% sort()
iris.train <- iris[.i,]
iris.test <- iris[setdiff(1:150, .i),]
plot(Sepal.Width ~ Sepal.Length, iris, type = "n")
points(Sepal.Width ~ Sepal.Length, iris.train, pch = as.integer(iris.train$Species))
points(Sepal.Width ~ Sepal.Length, iris.test[1,], pch = 16, cex = 2, col = 2)
points(Sepal.Width ~ Sepal.Length, iris.test[19,], pch = 16, cex = 2, col = 2)
points(Sepal.Width ~ Sepal.Length, iris.test[50,], pch = 16, cex = 2, col = 2)
```

--- &vcenter .largecontent

## K Nearest-Neighborhood (k-NN)

- 給一筆Testing data，找與它最近的前K個 Training data（鄰居）
- 用鄰居的類別中，出現次數最多的類別猜測Testing data的類別

--- &fullimg `r bg("279px-KnnClassification.png")`

--- &vcenter .largecontent

## k-NN 常用於評量Similarity

- k-NN 沒有太多的假設
    - 物以類聚
    - Similarity
- k-NN 的效果和Similarity 的挑選很關鍵
- <http://www.cs.ucr.edu/~eamonn/time_series_data/>

--- &vcenter .largecontent

## 練習

- 請完成RDataMining-03-Classification

--- .dark .segue

## Text Mining

--- &vcenter .largecontent

## 文字資料範例：ptt 笨版文章

```{r stupid-example}
dir("ptt-StupidClown/", ".*txt$", full.names = TRUE)[5] %>%
  readLines() %>%
  head(20) %>%
  cat(sep = "\n")
```

--- &vcenter .largecontent

## 文字資料的特色

- 容易獲取、俯拾即是
- 非結構化、長短不一、沒有明顯規律
    - 挖掘規律是個挑戰
    - 由各種字彙組成
    - 常用的資料分析技術不容易套用在文字資料上
    - 整理資料的挑戰較高

--- &vcenter .largecontent

## 文字資料的結構化

- 找出方法將非結構化的文章轉變成結構化的資料
- 後續可針對各種應用問題，與其他ML或DM方法結合

--- &vcenter .largecontent

## 文字資料的清理

- 移除不必要的字元，如空白、標點符號
- 統一大小寫
- 斷詞
    - 英文資料使用空白做切割
    - 中文資料可以使用Open Source斷詞引擎搭配詞庫
        - [g0v 萌典](https://www.moedict.tw/about.html)

--- &vcenter .largecontent

## Term Document Matrix (TDM)

- 將文字資料在斷詞後，轉換為結構化資料的方式
- 以文章為單位
    - 每篇文章是一筆資料
- 將文章中包含的詞彙當成屬性
    - 運用大量布林屬性來標註文章中有沒有包含特定的詞彙

--- &vcenter .largecontent

## Feature Hashing

- 一種加速TDM處理效能的技巧
    - TDM 需要建立：  字彙 ==> 屬性位置 的對應表
    - Feature Hashing 運用Hashing Algorithm來做對應
- 喪失對屬性的解釋力
- `r fig("300px-Hash_table_4_1_1_0_0_1_0_LL.png")`

--- &vcenter .largecontent

## 範例：Large Movie Review Dataset

- 50000 條關於電影的評論，來源： [IMDB](http://www.imdb.com)
- 一個電影最多不超過 30 reviews
- 將評論標記為「正面」與「負面」
    - 正負面是依據使用者的評分決定：分數小於等於4時是負面，其餘的是正面（滿分十分）
    - 50% 正面的評論，50% 負面的評論
- 這個資料可以自[Kaggle](https://www.kaggle.com/c/word2vec-nlp-tutorial)上下載

--- &vcenter .largecontent

## TDM

- `r fig("TDM.png")`

--- &vcenter .largecontent

## Sentiment Analysis via R, FeatureHashing and XGBoost

- 文章網址：<https://cran.r-project.org/web/packages/FeatureHashing/vignettes/SentimentAnalysis.html>
- 運用文章中介紹的技巧搭配Machine Learning套件，即可達到Benchmark的準確度

--- &vcenter .largecontent

## n-gram

- TDM 是標記字彙有無在文章之中
- n-gram 是將相鄰的n個字彙視為一個字彙

--- &vcenter .largecontent

## n-gram 範例

- `剛剛手上拿著手機在回人FB訊息回到一半，突然被我媽叫離開原本的位置，我媽找完我沒事後就忘記手機放在哪裡了`
- 斷詞：`剛剛  手上  拿  著  手機  在  回人  FB  訊息  回到  一半  突然  被  我媽  叫  離開  原本  的  位置  我媽  找  完  我  沒事  後  就  忘記  手機  放在  哪裡  了`
- TDM: 

```{r n-gram-exp1}
x <- strsplit("剛剛  手上  拿  著  手機  在  回人  FB  訊息  回到  一半  突然  被  我媽  叫  離開  原本  的  位置  我媽  找  完  我  沒事  後  就  忘記  手機  放在  哪裡  了", "  ")[[1]]
df <- matrix(1, ncol = 8)
colnames(df) <- head(x, 8)
kable(df)
```

--- &vcenter .largecontent

## n-gram 範例

- 斷詞：`剛剛  手上  拿  著  手機  在  回人  FB  訊息  回到  一半  突然  被  我媽  叫  離開  原本  的  位置  我媽  找  完  我  沒事  後  就  忘記  手機  放在  哪裡  了`
- 2-gram： `剛剛+手上  手上+拿  拿+著  著+手機  手機+在  在+回人  回人+FB  FB+訊息  訊息+回到  回到+一半  一半+突然  突然+被  被+我媽  我媽+叫  叫+離開  離開+原本  原本+的  的+位置  位置+我媽  我媽+找  找+完  完+我  我+沒事  沒事+後  後+就  就+忘記  忘記+手機  手機+放在  放在+哪裡  哪裡+了`
- TDM: 

```{r n-gram-exp2}
x2 <- 
  cbind(head(x, -1), tail(x, -1)) %>% 
  apply(1, function(x) paste(x, collapse = "+"))
df <- matrix(1, ncol = 8)
colnames(df) <- head(x2, 8)
kable(df)
```

--- &vcenter .largecontent

## GloVe Algorithm

- 給定大量的文章與單字
    - 利用機器學習的技術學習文章的結構
    - 將每一個單字轉換成一個數值向量
- 可能是目前相關演算法中最好的成果

--- &vcenter .largecontent

## GloVe Algorithm

- 可以到 <http://nlp.stanford.edu/projects/glove/> 下載 GloVe 的學習結果
- 使用Data Engineer的技巧讀取資料後，計算：
    1. 找出"paris", "french", "roman"的向量
    2. 計算`paris - french + roman`
    3. 尋找和計算結果最接近的點：

--- &vcenter .largecontent

## GloVe Algorithm

```{r import-glove, cache = TRUE, results="hide"}
library(data.table)
glove <- fread("glove.6B.50d.txt", header = FALSE, sep = " ", stringsAsFactors = FALSE, data.table = FALSE)
m <- glove[,-1] %>% as.matrix()
colnames(m) <- NULL
rownames(m) <- glove$V1
rm(glove);gc()
```

```{r glove, echo = TRUE, dependson="import-glove", cache = TRUE}
# m 是讀取GloVe專案的學習成果後的矩陣
library(FNN)
paris <- m["paris",]
french <- m["french",]
italy <- m["italy",]
query <- matrix(paris - french + italy, nrow = 1)
r <- get.knnx(m, query, k = 10)
rownames(m)[r$nn.index]
```

--- &vcenter .largecontent

## GloVe Algorithm

- 我們可以做一些奇怪的事情...

```{r glove-play, echo = TRUE, dependson="import-glove", cache = TRUE}
# m 是讀取GloVe專案的學習成果後的矩陣
library(FNN)
r <- get.knnx(m, m["gun",,drop = FALSE], k = 10)
rownames(m)[r$nn.index]
r <- get.knnx(m, m["warrior",,drop = FALSE], k = 10)
rownames(m)[r$nn.index]
query <- matrix(
  m["gun",] - m["police",] + m["warrior",]
  , nrow = 1)
r <- get.knnx(m, query, k = 10)
rownames(m)[r$nn.index]
```

--- &vcenter .largecontent

## 練習

- 請完成RDataMining-04-Text-Mining

--- &vcenter .largecontent

## Q&A

